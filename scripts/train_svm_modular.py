from __future__ import annotations

import json
import os
from collections import Counter

import joblib
import numpy as np
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.svm import SVC

from eeg_emotion.features.csv_stats import DEFAULT_CSV_FILES, build_tabular_dataset
from eeg_emotion.preprocess.pipeline import PreprocessConfig, SklearnPreprocessPipeline, augment_tabular
from eeg_emotion.utils.logging import setup_logging
from eeg_emotion.utils.paths import make_run_paths
from eeg_emotion.utils.seed import set_seed
from eeg_emotion.viz.confusion_matrix import save_confusion_matrix


def main() -> None:
    # -------------------- åŸºæœ¬é…ç½®ï¼ˆå…ˆå†™æ­»ï¼Œä¸‹ä¸€æ­¥å†ä¸Šyaml configï¼‰ -------------------- #
    set_seed(42)

    data_dir = "./data"  # ä½ çš„æ•°æ®æ ¹ç›®å½•ï¼šdata/<emotion>/sample*/xxx.csv
    emotions = ["happy", "sad", "normal"]
    csv_files = list(DEFAULT_CSV_FILES)

    run = make_run_paths(base_dir="outputs")
    logger = setup_logging(os.path.join(run.logs_dir, "train.log"))

    logger.info("ğŸ” Building dataset from %s", data_dir)
    X_all, y_all, skipped = build_tabular_dataset(data_dir=data_dir, emotions=emotions, csv_files=csv_files)

    logger.info("âœ… Samples: %d | Features: %d", X_all.shape[0], X_all.shape[1])
    logger.info("ğŸ“Š Label distribution: %s", dict(Counter(y_all)))
    if skipped:
        logger.info("âš ï¸ Skipped samples: %d (first 5 shown)", len(skipped))
        for s in skipped[:5]:
            logger.info("   - %s", s)

    # -------------------- åˆ’åˆ†ï¼šå…ˆ splitï¼Œå† fit é¢„å¤„ç†ï¼Œé¿å…ä¿¡æ¯æ³„æ¼ -------------------- #
    X_temp, X_test, y_temp, y_test = train_test_split(
        X_all, y_all, test_size=0.30, random_state=42, stratify=y_all
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.10, random_state=42, stratify=y_temp
    )
    logger.info("Split: train=%s val=%s test=%s", X_train.shape, X_val.shape, X_test.shape)

    # -------------------- é¢„å¤„ç†ï¼ˆfit on train onlyï¼‰ -------------------- #
    pp_cfg = PreprocessConfig(impute_strategy="mean", scale=True, pca_n_components=None, select_k_best=None)
    pp = SklearnPreprocessPipeline(pp_cfg)

    # æ³¨æ„ï¼šå¢å¼ºè¦å‘ç”Ÿåœ¨ impute ä¹‹åã€scale ä¹‹å‰è¿˜æ˜¯ä¹‹åï¼Ÿ
    # ä½ åŸè„šæœ¬æ˜¯ï¼šimpute -> augment -> scaleã€‚è¿™é‡Œä¿æŒä¸€è‡´ï¼š
    X_train_imputed = pp.imputer.fit_transform(X_train)  # imputerå•ç‹¬fit
    X_val_imputed = pp.imputer.transform(X_val)
    X_test_imputed = pp.imputer.transform(X_test)

    X_train_aug, y_train_aug = augment_tabular(X_train_imputed, y_train, noise_std=0.01, time_jitter=0.02)

    # scaler/select/pca åœ¨å¢å¼ºåçš„è®­ç»ƒé›† fit
    if pp.scaler is not None:
        X_train_scaled = pp.scaler.fit_transform(X_train_aug)
        X_val_scaled = pp.scaler.transform(X_val_imputed)
        X_test_scaled = pp.scaler.transform(X_test_imputed)
    else:
        X_train_scaled, X_val_scaled, X_test_scaled = X_train_aug, X_val_imputed, X_test_imputed

    # å¦‚æœä½ åé¢è¦åŠ  selectKBest / PCAï¼šå»ºè®®æŠŠâ€œaugmentåå†fitâ€ç»Ÿä¸€æ”¾è¿› pipeline.fit()
    pp._is_fit = True  # æ ‡è®°ä¸ºfitï¼Œç”¨äºtransformæ—¶ä¸æŠ¥é”™ï¼ˆæœ¬è„šæœ¬ç›®å‰åªç”¨åˆ°imputer/scalerï¼‰

    # -------------------- SVM ç½‘æ ¼æœç´¢ï¼ˆç¤ºä¾‹ï¼šå…ˆåšæœ€å°é›†åˆï¼‰ -------------------- #
    param_grid = {
        "C": [0.1, 1, 10],
        "gamma": ["scale", "auto"],
        "kernel": ["rbf", "poly", "sigmoid"],
        "class_weight": [None, "balanced"],
    }

    logger.info("â³ GridSearchCV for SVM...")
    grid = GridSearchCV(SVC(probability=True), param_grid, refit=True, cv=5, n_jobs=-1)
    grid.fit(X_train_scaled, y_train_aug)

    model: SVC = grid.best_estimator_
    logger.info("âœ… Best params: %s", grid.best_params_)

    # -------------------- æµ‹è¯•é›†è¯„ä¼° -------------------- #
    y_pred = model.predict(X_test_scaled)
    acc = float(accuracy_score(y_test, y_pred))
    report = classification_report(y_test, y_pred, target_names=emotions, output_dict=True)

    logger.info("ğŸ¯ Test accuracy: %.4f", acc)
    logger.info("\n" + classification_report(y_test, y_pred, target_names=emotions))

    # -------------------- äº§ç‰©ä¿å­˜ -------------------- #
    joblib.dump(model, os.path.join(run.models_dir, "svm.joblib"))

    # ä¿å­˜â€œå¯å¤ç”¨çš„é¢„å¤„ç†éƒ¨ä»¶â€ï¼ˆå…ˆä¿å­˜imputer+scalerï¼›ä¸‹ä¸€æ­¥æˆ‘ä»¬æŠŠaugmentä¹ŸæŠ½è±¡è¿›pipelineï¼‰
    joblib.dump({"imputer": pp.imputer, "scaler": pp.scaler, "cfg": pp_cfg}, os.path.join(run.artifacts_dir, "preprocess_parts.joblib"))

    save_confusion_matrix(
        y_true=y_test,
        y_pred=y_pred,
        class_names=emotions,
        save_path=os.path.join(run.figures_dir, "confusion_matrix.png"),
        normalize="true",
        title="SVM Confusion Matrix (Normalized)",
    )

    metrics = {
        "accuracy": acc,
        "best_params": grid.best_params_,
        "report": report,
    }
    with open(os.path.join(run.run_dir, "metrics.json"), "w", encoding="utf-8") as f:
        json.dump(metrics, f, ensure_ascii=False, indent=2)

    logger.info("âœ… Saved to: %s", run.run_dir)


if __name__ == "__main__":
    main()

# NOTE: æ¨èä½¿ç”¨ scripts/train.pyï¼ˆç¬¬äºŒæ­¥ï¼šæ›´å¹²å‡€çš„ preprocess + model adapterï¼‰ã€‚

# NOTE: Step 3 èµ·æ¨èä½¿ç”¨ï¼špython -m scripts.train -c configs/svm.yaml
